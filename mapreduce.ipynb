{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Daniel Acuna <deacuna@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- You can put the homework files anywhere you want in your http://notebook.acuna.io workspace but _do not change_ the file names. The TAs and the professor use these names to grade your homework.\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and TAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). \n",
    "- Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code creates the spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Map-Reduce: Gradient descent\n",
    "\n",
    "Throughout this assignment, you should use vanilla Python and not Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistical models $f(x)$ are learned by optimizing a loss function $L(\\Theta)$ that depends on a set of parameters $\\Theta$. There are several ways of finding the optimal $\\Theta$ for the loss function, one of which is to iteratively update following the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla L = \n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial L}{\\partial \\theta_0}\\\\ \n",
    "    \\frac{\\partial L}{\\partial \\theta_1} \\\\ \n",
    "    \\vdots\\\\ \n",
    "    \\frac{\\partial L}{\\partial \\theta_p}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "To then, compute the update\n",
    "$$\\Theta^{t+1} = \\Theta^t - \\eta \\nabla L$$\n",
    "\n",
    "Because we assume independence between data points, the gradient becomes a summation:\n",
    "\n",
    "$$\\nabla L = \\sum_{i=1}^{n} \\nabla L_i$$\n",
    "where $L_i$ is the loss function for the $i$-th data point.\n",
    "\n",
    "Take as example, the statistical model $f(x) = b_0 + b_1 x$ and loss function $L(\\Theta) = (f(x) - y)^2$. If we have a set of three datapoints $D=\\{ (x=1,y=2), (x=-2, y=-1), (x=4, y = 3)\\}$\n",
    "\n",
    "Then the loss function for each of them is\n",
    "$L_1 = \\left(b_{0} + b_{1} - 2\\right)^{2}$, \n",
    "$L_2 = \\left(b_{0} - 2 b_{1} + 1\\right)^{2}$, and\n",
    "$L_3 = \\left(b_{0} + 4 b_{1} - 3\\right)^{2}$\n",
    "\n",
    "with \n",
    "$$\\nabla L_i = \\left[\\begin{matrix}2 b_{0} + 2 b_{1} x_i - 2 y_i\\\\2 x_i \\left(b_{0} + b_{1} x_i - y_i\\right)\\end{matrix}\\right]$$\n",
    "\n",
    "if we start with a solution $b_0 = 0, b_1 = 1$, then the gradients are:\n",
    "\n",
    "$$\\nabla L_1 = \\left[\\begin{matrix}-2\\\\-2\\end{matrix}\\right]$$\n",
    "$$\\nabla L_2 = \\left[\\begin{matrix}-2\\\\4\\end{matrix}\\right]$$\n",
    "$$\\nabla L_3 = \\left[\\begin{matrix}2\\\\8\\end{matrix}\\right]$$\n",
    "\n",
    "which after accumulation would yield\n",
    "$$\\nabla L = \\left[\\begin{matrix}-2\\\\10\\end{matrix}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (5 pts)\n",
    "\n",
    "Create a function `f_linear(b, x)` that receives the parameters `b` and one data point `x` as lists and return the prediction for that data point. Assume that the first element of `b` is the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1ee003cc0679b4304ff2fa404558cd6f",
     "grade": false,
     "grade_id": "cell-e260b0944cc2cafb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define below the function `f_linear` which performs a linear prediction based on parameters as data point\n",
    "def f_linear(b, x):\n",
    "    # YOUR CODE HERE\n",
    "    res=b[0]\n",
    "    i=1\n",
    "    while i<len(b):\n",
    "        res=res+b[i]*x[i-1]\n",
    "        i=i+1\n",
    "    return res\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above, if we assume b = [0, 1], and the first data point x = [1], y = 2\n",
    "f_linear([0, 1], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4cdbdeeae72fba84e40d267674140347",
     "grade": true,
     "grade_id": "cell-e12a1ead7bf3835d",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test (5 pts)\n",
    "assert f_linear([1, 1, 2, 3], [2, 1, 3]) == 14\n",
    "assert f_linear([1], []) == 1\n",
    "assert f_linear([0, 1, 0, 1, 0, 1], [0, 10, 10, 10 , 10]) == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (5 pts)\n",
    "Define the function `L(y_pred, y)` that receives a prediction `y_pred` and the actual value `y` and returns the squared error between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7892a587f9bd66149dc1402292150369",
     "grade": false,
     "grade_id": "cell-b624ab75860c6e6e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def L(y_pred, y):\n",
    "    # YOUR CODE HERE\n",
    "    return (y_pred-y)**2\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there should be not error here\n",
    "L(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2^2 error\n",
    "L(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7857ffedf06e01569d1fa76de2392278",
     "grade": true,
     "grade_id": "cell-335fe2ab491bce32",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "assert L(1, 1) == 0\n",
    "assert L(0, 4) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (10 pts)\n",
    "Create a function `gf_linear(f, b, x, y)` which returns the gradient of the linear function `f` with parameter `b` with respect to the squared loss function, evaluated at `x` and the actual outcome `y`. This function should return a vector with each element $j$ corresponding to the gradient with respect $b_j$, with $j = \\{0, 1, \\dots, p\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f7329f34aa05e0b8ef30faedf9750a13",
     "grade": false,
     "grade_id": "cell-2d3493c9766da5c1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define `gf_linear`\n",
    "#∇𝐿𝑖=[2𝑏0+2𝑏1𝑥𝑖−2𝑦𝑖 2𝑥𝑖(𝑏0+𝑏1𝑥𝑖−𝑦𝑖)]\n",
    "def gf_linear(f, b, x, y):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #I am writing this code in a format, as general as possible\n",
    "    \n",
    "    #checking if I have 1).integers for 'b' and 'x' 2). Lists of granularity 1 (just 1 set of x-variables) 3). 'x' being a list of lists (more than 1 data points)\n",
    "    #For case 3, 'b' and 'y' would still be lists with granularity 1 however. My output in this case should be a list of lists; each internal list being gradient for that data point.\n",
    "    #Eg. op[1]=gradient for data point x[1] and y[1]. x[1] is a list though, thus there are more than 1 independent variables:\n",
    "    \n",
    "    #a). Checking for case 1. Gives a message for improper format, since 'b' will be intercept, no parameter for the input variable 'x':\n",
    "    if (str(type(b))==\"<class 'int'>\" and str(type(x))==\"<class 'int'>\"):\n",
    "        print(\"Wrong format; 'b' is the inetercept, but no parameter found for the input variable 'x'.\")\n",
    "        \n",
    "    #b). Checking if 'b' is just an int and 'x' is a list:\n",
    "    elif (str(type(b))==\"<class 'int'>\" and str(type(x))==\"<class 'list'>\"):\n",
    "        \n",
    "        #If 'x' is not an empty list, then I don't have a parameter for the 'x', just an intercept in the form of the integer 'b', so it is an error:\n",
    "        if len(x)>0:\n",
    "            print(\"More number of x-variables than parameters. Invalid format.\")\n",
    "    \n",
    "    #c). Checking for the case where I am having a list of parameters and just 1 input 'x' variable:\n",
    "    elif (str(type(b))==\"<class 'list'>\" and str(type(x))==\"<class 'int'>\"):\n",
    "        \n",
    "        #1). If I am having more than 2 parameters for this case, I'd have more parameters than x-variables, so an improper format:\n",
    "        if len(b)>2:\n",
    "            print(\"More number of parameters than x-variables. Invalid format.\")\n",
    "        \n",
    "        #2). If there is just 1 element in 'b', this will be a case similar to situation b). \n",
    "        elif len(b)==1:\n",
    "            print(\"Just intercept found; no parameter for input variable x.\")\n",
    "        \n",
    "        #3). If b is a list, but an empty one:\n",
    "        elif len(b)==0:\n",
    "            print(\"Parameter list is empty. Incorrect format.\")\n",
    "            \n",
    "        #4). Now I'd have 1 intercept and 1 parameter for the input variable 'x':\n",
    "        elif len(b)==2:\n",
    "            res=f(b,x)\n",
    "            fv_r=res-y\n",
    "            grad=[2*fv_r,2*fv_r*x]\n",
    "            return grad\n",
    "            \n",
    "    #d). If both 'b' and 'x' are lists:\n",
    "    elif (str(type(b))==\"<class 'list'>\" and str(type(x))==\"<class 'list'>\"):\n",
    "        grad=[]\n",
    "        if len(x)==0:\n",
    "            if len(b)==1:\n",
    "                res=f(b,x)\n",
    "                fv_r=res-y\n",
    "                return([2*fv_r])\n",
    "            else:\n",
    "                print(\"Cannot match number of parameters and number of input variables. 'x' is empty.\")\n",
    "        \n",
    "        elif type(x[0])==\"<class 'list'>\":\n",
    "            j=0\n",
    "            \n",
    "            while j<len(x):\n",
    "                if len(b)==len(x[j])+1:\n",
    "                    gr=[]\n",
    "                    res=f(b,x[j])\n",
    "                    fv_r=res-y[j]\n",
    "                    gr.append(2*fv_r)\n",
    "                    i=0\n",
    "                    while i<len(x[j]):\n",
    "                        gr.append(2*fv_r*x[j][i])\n",
    "                        i=i+1\n",
    "                    grad.append(gr)\n",
    "                else:\n",
    "                    print(\"Incorrect input; not enough parameters or x-variables in entry number \"+str(j+1))\n",
    "                    return -1\n",
    "                j=j+1\n",
    "        \n",
    "        elif str(type(x[0]))==\"<class 'int'>\":\n",
    "           \n",
    "            if len(b)==len(x)+1:\n",
    "                res=f(b,x)\n",
    "                \n",
    "                if str(type(y))==\"<class 'list'>\":\n",
    "                    fv_r=res-y[0]\n",
    "                elif str(type(y))==\"<class 'int'>\":\n",
    "                    fv_r=res-y\n",
    "                    \n",
    "                grad.append(2*fv_r)\n",
    "                i=0\n",
    "                while i<len(x):\n",
    "                    grad.append(2*fv_r*x[i])\n",
    "                    i=i+1\n",
    "            \n",
    "            else:\n",
    "                print(\"Incorrect input; not enough parameters or x-variables.\")\n",
    "                return -1\n",
    "        \n",
    "        return grad\n",
    "            \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2, -2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above and first data point\n",
    "x = [1]\n",
    "y = 2\n",
    "b = [0, 1]\n",
    "gf_linear(f_linear, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above and second data point\n",
    "x = [-2]\n",
    "y = -1\n",
    "b = [0, 1]\n",
    "gf_linear(f_linear, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcc6f859ba2c7ba8deb2a7232799d986",
     "grade": true,
     "grade_id": "cell-bc73ed6d8073de74",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## (10 pts)\n",
    "np.testing.assert_array_equal(gf_linear(f_linear, [0, 1], [1], 2), [-2, -2])\n",
    "np.testing.assert_array_equal(gf_linear(f_linear, [0, 1], [-2], -1), [-2, 4])\n",
    "np.testing.assert_array_equal(gf_linear(f_linear, [1], [], 0), [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (15 pts)\n",
    "\n",
    "Develop a map-reduce job that produces a value so that the first element of the value is the mean loss function across all the data. You might use other pieces of information as part of the value to create your computation.\n",
    "\n",
    "You will implement your map function as `map_mse(f, b, L, xy)` where `f` is the function `b` are the parameters of the function `L` is the loss function and `xy` is the data. Assume that the data will come as an RDD where each element is of the format:\n",
    "\n",
    "`[x, y]` where `x` is a list and `y` is a scalar.\n",
    "\n",
    "Since the key does not matter for this map reduce job, just put a constant of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9783bb5e9503f2ef935c7cb83c2fc43b",
     "grade": false,
     "grade_id": "cell-481c573523c2b098",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "rdd_data = sc.parallelize([\n",
    "    [[1, 2], 3],\n",
    "    [[3, 1], 4],\n",
    "    [[-1, 1.5], 0],\n",
    "    [[-9, 3], 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bb5b73b9485caf4962b2cef35a565c10",
     "grade": false,
     "grade_id": "cell-f87dee01323b2e82",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create function `map_mse` below\n",
    "def map_mse(f, b, L, xy):\n",
    "    # YOUR CODE HERE\n",
    "    x,y=xy\n",
    "    res=f(b,x)\n",
    "    return [0,[L(res,y),1]]\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [9, 1]], [0, [16, 1]], [0, [0.0, 1]], [0, [0, 1]]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should apply the map function in the following way:\n",
    "\n",
    "```python\n",
    "# for an example set of `b = [0, 0, 0]`\n",
    "rdd_data.map(lambda x: map_generator(f_linear, [0, 0, 0], L, x))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [9, 1]], [0, [16, 1]], [0, [0.0, 1]], [0, [0, 1]]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it here\n",
    "rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dbda664de2ce3dee744024f1bf661087",
     "grade": true,
     "grade_id": "cell-449c2d0c5f77a1af",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "assert rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).count() == 4\n",
    "assert rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).map(lambda x: len(x)).\\\n",
    "    distinct().\\\n",
    "    first() == 2\n",
    "\n",
    "assert rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).count() == 4\n",
    "# the first element should be a number\n",
    "assert isinstance((rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).first()[1][0]), \n",
    "                  (int, float, complex))\n",
    "# try with other initializations\n",
    "assert isinstance((rdd_data.map(lambda x: map_mse(f_linear, [1, 2, 3], L, x)).first()[1][0]), \n",
    "                  (int, float, complex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create a reduce job that receives two values of a previous reduce (or map) and merge them appropriately. Remember that at the end of the reduce job, the first element of the value should be the mean squared error. Create the function `reduce_mse(v1, v2)` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d4c7b7e49c6fcf2dbf3b3d525ef5e86d",
     "grade": false,
     "grade_id": "cell-664ff0abbe2fe932",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create function `reduce_mse` below\n",
    "def reduce_mse(v1, v2):\n",
    "    # YOUR CODE HERE\n",
    "    return [((v1[0]*v1[1])+(v2[0]*v2[1]))/(v1[1]+v2[1]),v1[1]+v2[1]]\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following function call should return the mean squared error\n",
    "rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.8125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following function call should return the mean squared error\n",
    "rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 2, 3], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ddfe12639adf21efa38a1554e59cdb37",
     "grade": true,
     "grade_id": "cell-39ff3270d4901c44",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "assert rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0] == 6.25\n",
    "\n",
    "assert rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0] == 3.25\n",
    "\n",
    "assert rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 2, 3], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0] == 41.8125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 pts)\n",
    "\n",
    "In this question, you will compute the cumulative gradient of a model on the data. You will define a map function `map_gradient(f, gf, b, xy)` that would receive a function `f`, its gradient `gf`, its parameters `b`, and a data point `xy = [x, y]`. Also you will define a function `reduce_gradient(v1, v2)` that combines the two values appropriately. In the map function, you probably do not need to keep extra values beyond the actual gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b5464a0f7f432697ee6d51651071c13",
     "grade": false,
     "grade_id": "cell-f6fa0087a0af6012",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the function `map_gradient` below\n",
    "def map_gradient(f, gf, b, xy):\n",
    "    # YOUR CODE HERE\n",
    "    x,y=xy\n",
    "    grad=gf(f,b,x,y)\n",
    "    return [0,grad]\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2783391c2092700ae7af8efe7da67aec",
     "grade": true,
     "grade_id": "cell-85e6b947e7eab1ca",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "assert len(rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)[1]).first()) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, [-10, -10, -30]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_gradient(f_linear, gf_linear, [0, 0, 0], [[1,3],5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ba1a1e364bf1c1610dba927c27ce4ca8",
     "grade": false,
     "grade_id": "cell-d3a5661b89fd6eec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the function `reduce_gradient` below\n",
    "def reduce_gradient(v1, v2):\n",
    "    # YOUR CODE HERE\n",
    "    red_grad=[]\n",
    "    i=0\n",
    "    while i<len(v1):\n",
    "        val=v1[i]+v2[i]\n",
    "        red_grad.append(val)\n",
    "        #red_grad[i]=v1[i]+v2[i]: This piece doesn't work. Always must use append()\n",
    "        i=i+1\n",
    "    return red_grad\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-14.0, -30.0, -20.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)).\\\n",
    "    reduceByKey(reduce_gradient).first()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed8453696d68f53bc7e3d30156f7f8da",
     "grade": true,
     "grade_id": "cell-7f50da09ee10e7d1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal(\n",
    "    rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)).\\\n",
    "    reduceByKey(reduce_gradient).first()[1],\n",
    "    [-14.0, -30.0, -20.0])\n",
    "\n",
    "np.testing.assert_array_equal(\n",
    "    rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)).\\\n",
    "    reduceByKey(reduce_gradient).first()[1],\n",
    "    [-14.0, -30.0, -20.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if all your answers are correct, then we can run an optimization below, and the MSE should decrease with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.14, 0.3, 0.2]\n",
      "Current MSE: \t\t 4.036099999999999\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.27480000000000004, 0.15880000000000005, 0.455]\n",
      "Current MSE: \t\t 2.8077335025\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.34362200000000004, 0.41343399999999997, 0.540541]\n",
      "Current MSE: \t\t 2.124745162297563\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.42466317000000003, 0.24800435, 0.707635855]\n",
      "Current MSE: \t\t 1.7435970621414336\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.45430526015000006, 0.47522477825, 0.730516771125]\n",
      "Current MSE: \t\t 1.529538732230305\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.50541029705925, 0.29867069991675, 0.848308677264375]\n",
      "Current MSE: \t\t 1.4080112827808984\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5135716556948637, 0.5084709260312962, 0.8371720415554381]\n",
      "Current MSE: \t\t 1.3377595648707117\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5479266281297145, 0.32798388034815074, 0.9270367149304003]\n",
      "Current MSE: \t\t 1.2959552690942715\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5443950562815554, 0.5259519919004072, 0.8977132121221939]\n",
      "Current MSE: \t\t 1.2699656129248922\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5693007089887507, 0.34545294325487574, 0.971494595933439]\n",
      "Current MSE: \t\t 1.252798997362518\n"
     ]
    }
   ],
   "source": [
    "b = [0, 0, 0]\n",
    "learning_rate = 0.01\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** In the code, above, play with the value of `learning_rate` less than 1.0 until the optimizer diverges (the loss function goes down and then goes *up*). What is this learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc837820a5a204f6d39c8fd064139e4b",
     "grade": true,
     "grade_id": "cell-49ee8798a623ecdf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.1246, 0.267, 0.178]\n",
      "Current MSE: \t\t 4.1302238099999995\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.24508108, 0.18452548, 0.3995655]\n",
      "Current MSE: \t\t 2.867466506436631\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.318596634118, 0.34265946194600005, 0.506459863229]\n",
      "Current MSE: \t\t 2.115207415373838\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.38949619256355966, 0.2943986083757235, 0.6387681500672706]\n",
      "Current MSE: \t\t 1.6670371339312942\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.43253028699358087, 0.3880863578385662, 0.7031536588366425]\n",
      "Current MSE: \t\t 1.4000046583047048\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.473910740122105, 0.35987954838441216, 0.7823601931883728]\n",
      "Current MSE: \t\t 1.2408719109427595\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.49875834540221853, 0.415415735805836, 0.8213330385915345]\n",
      "Current MSE: \t\t 1.1460130288731225\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.522565191141674, 0.3989619281819948, 0.8689466203553182]\n",
      "Current MSE: \t\t 1.0894411795892902\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5365633096447888, 0.43191220829972055, 0.8927271319586574]\n",
      "Current MSE: \t\t 1.0556765800396168\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5499091537280093, 0.422345496756468, 0.9215434839338096]\n",
      "Current MSE: \t\t 1.035498218250381\n"
     ]
    }
   ],
   "source": [
    "b = [0, 0, 0]\n",
    "learning_rate = 0.0089\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.126, 0.26999999999999996, 0.18]\n",
      "Current MSE: \t\t 4.120141\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.247788, 0.182628, 0.40454999999999997]\n",
      "Current MSE: \t\t 2.85570663639025\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.321056838, 0.34808178599999995, 0.510019389]\n",
      "Current MSE: \t\t 2.1049292297785462\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.392680961037, 0.29207068843500006, 0.6447346400655]\n",
      "Current MSE: \t\t 1.6590588708536282\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.4349123897844735, 0.3935339249263425, 0.7066837293143512]\n",
      "Current MSE: \t\t 1.394203448815854\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.476698058154599, 0.35773950084501305, 0.7877304161128424]\n",
      "Current MSE: \t\t 1.2368252572203948\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5006680578834956, 0.42001624374210045, 0.8242888404064057]\n",
      "Current MSE: \t\t 1.1432699247936993\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5247027185851659, 0.39721567384850914, 0.8732610378761687]\n",
      "Current MSE: \t\t 1.0876196079919576\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5379331755093902, 0.4354838305841953, 0.8950195473263113]\n",
      "Current MSE: \t\t 1.0544847616390465\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5514066016867553, 0.4210126586108117, 0.9248150113767122]\n",
      "Current MSE: \t\t 1.0347261282093798\n"
     ]
    }
   ],
   "source": [
    "b = [0, 0, 0]\n",
    "learning_rate = 0.009\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [4.2, 9.0, 6.0]\n",
      "Current MSE: \t\t 1267.54\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [3.7199999999999935, -379.08, 61.49999999999998]\n",
      "Current MSE: \t\t 3602067.069024999\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-1642.4459999999997, 21435.677999999993, -5893.893]\n",
      "Current MSE: \t\t 12338338354.600441\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [105994.58369999996, -1250821.4444999993, 361211.6305499999]\n",
      "Current MSE: \t\t 42376138027663.06\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-6276797.754854997, 73269195.78397495, -21274153.761412486]\n",
      "Current MSE: \t\t 1.4554630449581718e+17\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [368290317.8054631, -4293752442.444835, 1247490101.8632536]\n",
      "Current MSE: \t\t 4.9989775679427615e+20\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-21586820691.9137, 251636837969.8816, -73114754253.90024]\n",
      "Current MSE: \t\t 1.7169640245103839e+24\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [1265130559807.0037, -14747347207429.465, 4284974208216.569]\n",
      "Current MSE: \t\t 5.897136807721217e+27\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [-74144016667446.25, 864278824993844.9, -251124207465775.97]\n",
      "Current MSE: \t\t 2.0254485261525482e+31\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [4345264326908262.0, -5.065168209993662e+16, 1.4717316322742266e+16]\n",
      "Current MSE: \t\t 6.956667050223745e+34\n"
     ]
    }
   ],
   "source": [
    "b = [0, 0, 0]\n",
    "learning_rate = 0.3\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple executions of the same code, but with different learning rate values, showed me that the lowest Mean Squared Error was encountered for a learning rate of 0.009. As I tried moving the learning rate in either directions of 0.009, the Mean Squared Error went up. I can thus conclude that the learning curve is probably a parabolic curve, with the minima at the value of 0.009 (roughly) for the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
